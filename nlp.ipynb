{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3823d731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loading dataset...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/final_main_dataset.tsv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 659\u001b[0m\n\u001b[0;32m    655\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[0;32m    658\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 659\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 591\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    588\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing device: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    590\u001b[0m \u001b[38;5;66;03m# Prepare data\u001b[39;00m\n\u001b[1;32m--> 591\u001b[0m train_data, val_data, test_data, vocab \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata/final_main_dataset.tsv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    593\u001b[0m \u001b[38;5;66;03m# Create datasets\u001b[39;00m\n\u001b[0;32m    594\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m UrduQADataset(train_data, vocab, MAX_LEN)\n",
      "Cell \u001b[1;32mIn[2], line 526\u001b[0m, in \u001b[0;36mprepare_data\u001b[1;34m(tsv_path, test_mode)\u001b[0m\n\u001b[0;32m    524\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load and prepare dataset\"\"\"\u001b[39;00m\n\u001b[0;32m    525\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading dataset...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 526\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtsv_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\t\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    528\u001b[0m \u001b[38;5;66;03m# Extract sentences from column C\u001b[39;00m\n\u001b[0;32m    529\u001b[0m sentences \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdropna()\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[1;32mc:\\Users\\Yashal Rafique\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Yashal Rafique\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\Yashal Rafique\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Yashal Rafique\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\Yashal Rafique\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/final_main_dataset.tsv'"
     ]
    }
   ],
   "source": [
    "# ===== URDU CONVERSATIONAL CHATBOT WITH SPAN CORRUPTION =====\n",
    "# Complete Implementation for Question-Answering Task\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "import math\n",
    "from typing import List, Tuple\n",
    "from collections import Counter\n",
    "import pickle\n",
    "\n",
    "# ==================== DATA PREPROCESSING ====================\n",
    "\n",
    "class UrduTextPreprocessor:\n",
    "    \"\"\"Preprocess and normalize Urdu text\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def normalize_urdu(text: str) -> str:\n",
    "        \"\"\"Normalize Urdu text by removing diacritics and standardizing characters\"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "        \n",
    "        # Remove diacritics (zabar, zer, pesh, etc.)\n",
    "        text = re.sub(r'[\\u064B-\\u065F]', '', text)\n",
    "        \n",
    "        # Standardize Alef forms\n",
    "        text = text.replace('ٱ', 'ا').replace('أ', 'ا').replace('إ', 'ا').replace('آ', 'ا')\n",
    "        \n",
    "        # Standardize Yeh forms\n",
    "        text = text.replace('ى', 'ی').replace('ئ', 'ی')\n",
    "        \n",
    "        # Standardize Heh forms\n",
    "        text = text.replace('ۃ', 'ہ').replace('ھ', 'ہ')\n",
    "        \n",
    "        # Remove extra whitespaces\n",
    "        text = ' '.join(text.split())\n",
    "        \n",
    "        return text.strip()\n",
    "    \n",
    "    @staticmethod\n",
    "    def span_corruption(sentence: str, mask_ratio: float = 0.15, \n",
    "                       max_span_length: int = 3) -> Tuple[str, str]:\n",
    "        \"\"\"\n",
    "        Apply span corruption to create input-output pairs for QA task\n",
    "        \n",
    "        Args:\n",
    "            sentence: Original Urdu sentence\n",
    "            mask_ratio: Percentage of tokens to mask (default 15%)\n",
    "            max_span_length: Maximum consecutive tokens to mask\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (corrupted_text, original_text)\n",
    "        \"\"\"\n",
    "        words = sentence.split()\n",
    "        \n",
    "        if len(words) < 3:\n",
    "            return None, None\n",
    "        \n",
    "        # Calculate number of spans to mask\n",
    "        num_tokens_to_mask = max(1, int(len(words) * mask_ratio))\n",
    "        \n",
    "        corrupted = words.copy()\n",
    "        masked_positions = set()\n",
    "        \n",
    "        attempts = 0\n",
    "        while len(masked_positions) < num_tokens_to_mask and attempts < 50:\n",
    "            # Random span length (1 to max_span_length)\n",
    "            span_len = random.randint(1, min(max_span_length, len(words)))\n",
    "            \n",
    "            # Random start position\n",
    "            if len(words) < span_len:\n",
    "                break\n",
    "                \n",
    "            start_idx = random.randint(0, len(words) - span_len)\n",
    "            \n",
    "            # Check if positions are not already masked\n",
    "            new_positions = set(range(start_idx, start_idx + span_len))\n",
    "            if not new_positions.intersection(masked_positions):\n",
    "                for i in new_positions:\n",
    "                    corrupted[i] = '[MASK]'\n",
    "                    masked_positions.add(i)\n",
    "            \n",
    "            attempts += 1\n",
    "        \n",
    "        # Clean up consecutive [MASK] tokens\n",
    "        cleaned = []\n",
    "        prev_mask = False\n",
    "        for token in corrupted:\n",
    "            if token == '[MASK]':\n",
    "                if not prev_mask:\n",
    "                    cleaned.append(token)\n",
    "                prev_mask = True\n",
    "            else:\n",
    "                cleaned.append(token)\n",
    "                prev_mask = False\n",
    "        \n",
    "        return ' '.join(cleaned), sentence\n",
    "\n",
    "\n",
    "class Vocabulary:\n",
    "    \"\"\"Build and manage vocabulary for Urdu text\"\"\"\n",
    "    \n",
    "    def __init__(self, min_freq: int = 2):\n",
    "        self.word2idx = {\n",
    "            '<PAD>': 0,\n",
    "            '<SOS>': 1,\n",
    "            '<EOS>': 2,\n",
    "            '<UNK>': 3,\n",
    "            '[MASK]': 4\n",
    "        }\n",
    "        self.idx2word = {v: k for k, v in self.word2idx.items()}\n",
    "        self.word_count = Counter()\n",
    "        self.min_freq = min_freq\n",
    "        self.n_words = 5\n",
    "    \n",
    "    def add_sentence(self, sentence: str):\n",
    "        \"\"\"Add all words in sentence to vocabulary\"\"\"\n",
    "        for word in sentence.split():\n",
    "            self.word_count[word] += 1\n",
    "    \n",
    "    def build_vocab(self):\n",
    "        \"\"\"Build final vocabulary based on word frequency\"\"\"\n",
    "        for word, count in self.word_count.items():\n",
    "            if count >= self.min_freq and word not in self.word2idx:\n",
    "                self.word2idx[word] = self.n_words\n",
    "                self.idx2word[self.n_words] = word\n",
    "                self.n_words += 1\n",
    "    \n",
    "    def encode(self, sentence: str, max_len: int = None) -> List[int]:\n",
    "        \"\"\"Convert sentence to indices\"\"\"\n",
    "        tokens = [self.word2idx.get(word, self.word2idx['<UNK>']) \n",
    "                 for word in sentence.split()]\n",
    "        \n",
    "        if max_len:\n",
    "            tokens = tokens[:max_len]\n",
    "            \n",
    "        return [self.word2idx['<SOS>']] + tokens + [self.word2idx['<EOS>']]\n",
    "    \n",
    "    def decode(self, indices: List[int]) -> str:\n",
    "        \"\"\"Convert indices back to sentence\"\"\"\n",
    "        words = []\n",
    "        for idx in indices:\n",
    "            if idx == self.word2idx['<EOS>']:\n",
    "                break\n",
    "            if idx not in [self.word2idx['<PAD>'], self.word2idx['<SOS>']]:\n",
    "                words.append(self.idx2word.get(idx, '<UNK>'))\n",
    "        return ' '.join(words)\n",
    "\n",
    "\n",
    "class UrduQADataset(Dataset):\n",
    "    \"\"\"Dataset class for Urdu QA with span corruption\"\"\"\n",
    "    \n",
    "    def __init__(self, data_pairs: List[Tuple[str, str]], vocab: Vocabulary, \n",
    "                 max_len: int = 50):\n",
    "        self.data = data_pairs\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        src, tgt = self.data[idx]\n",
    "        \n",
    "        src_indices = self.vocab.encode(src, self.max_len)\n",
    "        tgt_indices = self.vocab.encode(tgt, self.max_len)\n",
    "        \n",
    "        # Pad sequences\n",
    "        src_padded = src_indices + [0] * (self.max_len + 2 - len(src_indices))\n",
    "        tgt_padded = tgt_indices + [0] * (self.max_len + 2 - len(tgt_indices))\n",
    "        \n",
    "        return torch.tensor(src_padded[:self.max_len + 2]), \\\n",
    "               torch.tensor(tgt_padded[:self.max_len + 2])\n",
    "\n",
    "\n",
    "# ==================== MODEL ARCHITECTURE ====================\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Positional encoding for transformer\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, max_len: int = 5000, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Create positional encoding matrix\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n",
    "                            (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multi-head attention mechanism\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, num_heads: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        # Linear layers for Q, K, V\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        \"\"\"Calculate scaled dot-product attention\"\"\"\n",
    "        # Q, K, V: (batch_size, num_heads, seq_len, d_k)\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "        attn_probs = self.dropout(attn_probs)\n",
    "        \n",
    "        output = torch.matmul(attn_probs, V)\n",
    "        return output, attn_probs\n",
    "    \n",
    "    def split_heads(self, x):\n",
    "        \"\"\"Split into multiple heads\"\"\"\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
    "    \n",
    "    def combine_heads(self, x):\n",
    "        \"\"\"Combine multiple heads\"\"\"\n",
    "        batch_size, _, seq_length, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "    \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        # Apply linear transformations and split heads\n",
    "        Q = self.split_heads(self.W_q(Q))\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "        \n",
    "        # Apply attention\n",
    "        attn_output, attn_probs = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        \n",
    "        # Combine heads and apply output linear\n",
    "        output = self.W_o(self.combine_heads(attn_output))\n",
    "        return output, attn_probs\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"Position-wise feed-forward network\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear2(self.dropout(self.relu(self.linear1(x))))\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"Single transformer encoder layer\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, num_heads: int, d_ff: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        # Self-attention with residual connection\n",
    "        attn_output, _ = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        \n",
    "        # Feed-forward with residual connection\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    \"\"\"Single transformer decoder layer\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, num_heads: int, d_ff: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, enc_output, src_mask=None, tgt_mask=None):\n",
    "        # Masked self-attention\n",
    "        attn_output, _ = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        \n",
    "        # Cross-attention with encoder output\n",
    "        attn_output, _ = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
    "        x = self.norm2(x + self.dropout(attn_output))\n",
    "        \n",
    "        # Feed-forward\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    \"\"\"Complete Transformer model for Urdu QA\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size: int, d_model: int = 256, num_heads: int = 2,\n",
    "                 num_encoder_layers: int = 2, num_decoder_layers: int = 2,\n",
    "                 d_ff: int = 1024, dropout: float = 0.1, max_len: int = 52):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        # Embeddings\n",
    "        self.encoder_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.decoder_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_len, dropout)\n",
    "        \n",
    "        # Encoder layers\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            EncoderLayer(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_encoder_layers)\n",
    "        ])\n",
    "        \n",
    "        # Decoder layers\n",
    "        self.decoder_layers = nn.ModuleList([\n",
    "            DecoderLayer(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_decoder_layers)\n",
    "        ])\n",
    "        \n",
    "        # Output projection\n",
    "        self.output_linear = nn.Linear(d_model, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize model weights\"\"\"\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "    \n",
    "    def create_padding_mask(self, seq):\n",
    "        \"\"\"Create mask for padding tokens\"\"\"\n",
    "        return (seq != 0).unsqueeze(1).unsqueeze(2)\n",
    "    \n",
    "    def create_look_ahead_mask(self, size):\n",
    "        \"\"\"Create mask to prevent attending to future tokens\"\"\"\n",
    "        mask = torch.triu(torch.ones(size, size), diagonal=1).type(torch.uint8)\n",
    "        return mask == 0\n",
    "    \n",
    "    def encode(self, src, src_mask):\n",
    "        \"\"\"Encode source sequence\"\"\"\n",
    "        # Embedding + positional encoding\n",
    "        x = self.encoder_embedding(src) * math.sqrt(self.d_model)\n",
    "        x = self.pos_encoding(x)\n",
    "        \n",
    "        # Pass through encoder layers\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x, src_mask)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def decode(self, tgt, enc_output, src_mask, tgt_mask):\n",
    "        \"\"\"Decode target sequence\"\"\"\n",
    "        # Embedding + positional encoding\n",
    "        x = self.decoder_embedding(tgt) * math.sqrt(self.d_model)\n",
    "        x = self.pos_encoding(x)\n",
    "        \n",
    "        # Pass through decoder layers\n",
    "        for layer in self.decoder_layers:\n",
    "            x = layer(x, enc_output, src_mask, tgt_mask)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def forward(self, src, tgt):\n",
    "        \"\"\"Forward pass\"\"\"\n",
    "        # Create masks\n",
    "        src_mask = self.create_padding_mask(src)\n",
    "        tgt_mask = self.create_padding_mask(tgt) & \\\n",
    "                   self.create_look_ahead_mask(tgt.size(1)).to(tgt.device)\n",
    "        \n",
    "        # Encode and decode\n",
    "        enc_output = self.encode(src, src_mask)\n",
    "        dec_output = self.decode(tgt, enc_output, src_mask, tgt_mask)\n",
    "        \n",
    "        # Project to vocabulary\n",
    "        output = self.output_linear(dec_output)\n",
    "        return output\n",
    "    \n",
    "    def generate(self, src, vocab, max_len=50, strategy='greedy', beam_size=3):\n",
    "        \"\"\"Generate response using greedy or beam search\"\"\"\n",
    "        self.eval()\n",
    "        device = src.device\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Encode source\n",
    "            src_mask = self.create_padding_mask(src)\n",
    "            enc_output = self.encode(src, src_mask)\n",
    "            \n",
    "            # Initialize decoder input\n",
    "            tgt = torch.tensor([[vocab.word2idx['<SOS>']]]).to(device)\n",
    "            \n",
    "            if strategy == 'greedy':\n",
    "                return self._greedy_decode(tgt, enc_output, src_mask, vocab, max_len)\n",
    "            else:\n",
    "                return self._beam_search(tgt, enc_output, src_mask, vocab, max_len, beam_size)\n",
    "    \n",
    "    def _greedy_decode(self, tgt, enc_output, src_mask, vocab, max_len):\n",
    "        \"\"\"Greedy decoding\"\"\"\n",
    "        for _ in range(max_len):\n",
    "            tgt_mask = self.create_padding_mask(tgt) & \\\n",
    "                      self.create_look_ahead_mask(tgt.size(1)).to(tgt.device)\n",
    "            \n",
    "            dec_output = self.decode(tgt, enc_output, src_mask, tgt_mask)\n",
    "            output = self.output_linear(dec_output)\n",
    "            \n",
    "            next_token = output[:, -1, :].argmax(dim=-1).unsqueeze(0)\n",
    "            tgt = torch.cat([tgt, next_token], dim=1)\n",
    "            \n",
    "            if next_token.item() == vocab.word2idx['<EOS>']:\n",
    "                break\n",
    "        \n",
    "        return tgt.squeeze(0).tolist()\n",
    "\n",
    "\n",
    "# ==================== TRAINING ====================\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, criterion, device, clip=1.0):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch_idx, (src, tgt) in enumerate(dataloader):\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "        \n",
    "        # Prepare decoder input and output (teacher forcing)\n",
    "        tgt_input = tgt[:, :-1]\n",
    "        tgt_output = tgt[:, 1:]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(src, tgt_input)\n",
    "        \n",
    "        # Reshape for loss calculation\n",
    "        output = output.reshape(-1, output.shape[-1])\n",
    "        tgt_output = tgt_output.reshape(-1)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(output, tgt_output)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if batch_idx % 50 == 0:\n",
    "            print(f'  Batch {batch_idx}/{len(dataloader)}, Loss: {loss.item():.4f}')\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    \"\"\"Evaluate model\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for src, tgt in dataloader:\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            \n",
    "            tgt_input = tgt[:, :-1]\n",
    "            tgt_output = tgt[:, 1:]\n",
    "            \n",
    "            output = model(src, tgt_input)\n",
    "            output = output.reshape(-1, output.shape[-1])\n",
    "            tgt_output = tgt_output.reshape(-1)\n",
    "            \n",
    "            loss = criterion(output, tgt_output)\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "# ==================== MAIN PIPELINE ====================\n",
    "\n",
    "def prepare_data(tsv_path: str, test_mode=False):\n",
    "    \"\"\"Load and prepare dataset\"\"\"\n",
    "    print(\"Loading dataset...\")\n",
    "    df = pd.read_csv(tsv_path, sep='\\t')\n",
    "    \n",
    "    # Extract sentences from column C\n",
    "    sentences = df['sentence'].dropna().tolist()\n",
    "    \n",
    "    if test_mode:\n",
    "        sentences = sentences[:1000]  # Use subset for testing\n",
    "    \n",
    "    print(f\"Total sentences: {len(sentences)}\")\n",
    "    \n",
    "    # Normalize text\n",
    "    preprocessor = UrduTextPreprocessor()\n",
    "    sentences = [preprocessor.normalize_urdu(s) for s in sentences if s.strip()]\n",
    "    \n",
    "    # Apply span corruption\n",
    "    print(\"Applying span corruption...\")\n",
    "    data_pairs = []\n",
    "    for sent in sentences:\n",
    "        corrupted, original = preprocessor.span_corruption(sent, mask_ratio=0.15)\n",
    "        if corrupted and original:\n",
    "            data_pairs.append((corrupted, original))\n",
    "    \n",
    "    print(f\"Created {len(data_pairs)} training pairs\")\n",
    "    \n",
    "    # Build vocabulary\n",
    "    print(\"Building vocabulary...\")\n",
    "    vocab = Vocabulary(min_freq=2)\n",
    "    for src, tgt in data_pairs:\n",
    "        vocab.add_sentence(src)\n",
    "        vocab.add_sentence(tgt)\n",
    "    vocab.build_vocab()\n",
    "    \n",
    "    print(f\"Vocabulary size: {vocab.n_words}\")\n",
    "    \n",
    "    # Split data\n",
    "    split1 = int(0.8 * len(data_pairs))\n",
    "    split2 = int(0.9 * len(data_pairs))\n",
    "    \n",
    "    train_data = data_pairs[:split1]\n",
    "    val_data = data_pairs[split1:split2]\n",
    "    test_data = data_pairs[split2:]\n",
    "    \n",
    "    print(f\"Train: {len(train_data)}, Val: {len(val_data)}, Test: {len(test_data)}\")\n",
    "    \n",
    "    return train_data, val_data, test_data, vocab\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main training pipeline\"\"\"\n",
    "    # Hyperparameters\n",
    "    BATCH_SIZE = 32\n",
    "    EPOCHS = 20\n",
    "    D_MODEL = 256\n",
    "    NUM_HEADS = 2\n",
    "    NUM_ENCODER_LAYERS = 2\n",
    "    NUM_DECODER_LAYERS = 2\n",
    "    D_FF = 1024\n",
    "    DROPOUT = 0.1\n",
    "    LEARNING_RATE = 3e-4\n",
    "    MAX_LEN = 50\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Prepare data\n",
    "    train_data, val_data, test_data, vocab = prepare_data('data/final_main_dataset.tsv', test_mode=False)\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = UrduQADataset(train_data, vocab, MAX_LEN)\n",
    "    val_dataset = UrduQADataset(val_data, vocab, MAX_LEN)\n",
    "    test_dataset = UrduQADataset(test_data, vocab, MAX_LEN)\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = Transformer(\n",
    "        vocab_size=vocab.n_words,\n",
    "        d_model=D_MODEL,\n",
    "        num_heads=NUM_HEADS,\n",
    "        num_encoder_layers=NUM_ENCODER_LAYERS,\n",
    "        num_decoder_layers=NUM_DECODER_LAYERS,\n",
    "        d_ff=D_FF,\n",
    "        dropout=DROPOUT,\n",
    "        max_len=MAX_LEN + 2\n",
    "    ).to(device)\n",
    "    \n",
    "    print(f\"\\nModel parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    # Optimizer and loss\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)  # Ignore padding\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Starting Training...\")\n",
    "    print(\"=\"*50 + \"\\n\")\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
    "        \n",
    "        train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        val_loss = evaluate(model, val_loader, criterion, device)\n",
    "        \n",
    "        print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_loss': val_loss,\n",
    "            }, 'best_urdu_chatbot.pt')\n",
    "            print(\"✓ Model saved!\")\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    # Save vocabulary\n",
    "    with open('vocab.pkl', 'wb') as f:\n",
    "        pickle.dump(vocab, f)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Training Complete!\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
